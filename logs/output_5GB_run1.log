2025-05-07 23:17:10,798 - INFO - NVTabular version: 23.08.00
2025-05-07 23:17:10,798 - INFO - cuDF version: 23.04.01
2025-05-07 23:17:10,798 - INFO - Pandas version: 1.5.3
2025-05-07 23:17:10,800 - INFO - visible_devices: 0
2025-05-07 23:17:10,800 - INFO - Device size: 25.33 GB
2025-05-07 23:17:10,800 - INFO - Device limit: 17.73 GB
2025-05-07 23:17:10,800 - INFO - Device pool size: 20.26 GB
2025-05-07 23:17:11,268 - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2025-05-07 23:17:11,302 - INFO - State start
2025-05-07 23:17:11,311 - INFO -   Scheduler at:     tcp://127.0.0.1:42703
2025-05-07 23:17:11,312 - INFO -   dashboard at:  http://127.0.0.1:8787/status
2025-05-07 23:17:11,402 - INFO -         Start Nanny at: 'tcp://127.0.0.1:34733'
2025-05-07 23:17:16,789 - INFO - NVTabular version: 23.08.00
2025-05-07 23:17:16,789 - INFO - cuDF version: 23.04.01
2025-05-07 23:17:16,789 - INFO - Pandas version: 1.5.3
2025-05-07 23:17:16,793 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2025-05-07 23:17:16,793 - INFO - Creating preload: dask_cuda.initialize
2025-05-07 23:17:16,793 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2025-05-07 23:17:16,793 - INFO - Import preload module: dask_cuda.initialize
2025-05-07 23:17:16,814 - INFO - Run preload setup: dask_cuda.initialize
2025-05-07 23:17:16,985 - INFO -       Start worker at:      tcp://127.0.0.1:44071
2025-05-07 23:17:16,985 - INFO -          Listening to:      tcp://127.0.0.1:44071
2025-05-07 23:17:16,985 - INFO -           Worker name:                          0
2025-05-07 23:17:16,985 - INFO -          dashboard at:            127.0.0.1:43403
2025-05-07 23:17:16,985 - INFO - Waiting to connect to:      tcp://127.0.0.1:42703
2025-05-07 23:17:16,985 - INFO - -------------------------------------------------
2025-05-07 23:17:16,985 - INFO -               Threads:                          1
2025-05-07 23:17:16,985 - INFO -                Memory:                 251.74 GiB
2025-05-07 23:17:16,985 - INFO -       Local Directory: /tmp/dask-worker-space/worker-4ibcxju7
2025-05-07 23:17:16,985 - INFO - Starting Worker plugin RMMSetup-4e3ce9a8-2419-4213-b61b-2c98702b188a
2025-05-07 23:17:17,587 - INFO - Starting Worker plugin PreImport-c53f309c-fdde-47d5-a9e5-f7a27141d484
2025-05-07 23:17:17,587 - INFO - Starting Worker plugin CPUAffinity-112929f2-ecfd-4964-85cc-d62725e313b7
2025-05-07 23:17:17,588 - INFO - -------------------------------------------------
2025-05-07 23:17:17,611 - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44071', name: 0, status: init, memory: 0, processing: 0>
2025-05-07 23:17:17,633 - INFO - Starting worker compute stream, tcp://127.0.0.1:44071
2025-05-07 23:17:17,633 - INFO - Starting established connection to tcp://127.0.0.1:35434
2025-05-07 23:17:17,633 - INFO -         Registered to:      tcp://127.0.0.1:42703
2025-05-07 23:17:17,633 - INFO - -------------------------------------------------
2025-05-07 23:17:17,634 - INFO - Starting established connection to tcp://127.0.0.1:42703
2025-05-07 23:17:17,690 - INFO - Receive client connection: Client-a7a8287f-2b88-11f0-b518-506b4bdbc6c4
2025-05-07 23:17:17,690 - INFO - Starting established connection to tcp://127.0.0.1:35448
2025-05-07 23:17:17,694 - INFO - Found 1024 training files
2025-05-07 23:17:17,694 - INFO - First file: /mnt/scratch/yuzhuyu/parquet/criteo_1TB/criteo_1TB_part_0991.parquet
2025-05-07 23:17:17,694 - INFO - Last file: /mnt/scratch/yuzhuyu/parquet/criteo_1TB/criteo_1TB_part_0855.parquet
2025-05-07 23:17:18,041 - INFO - Total data size: 1473.35 GB
2025-05-07 23:17:18,041 - INFO - Setting up workflow...
2025-05-07 23:17:18,043 - INFO - Creating dataset...
2025-05-07 23:17:23,784 - INFO - Dataset created successfully with cuDF engine
2025-05-07 23:17:23,785 - INFO - Part size: 5GB
2025-05-07 23:17:23,785 - INFO - Fitting workflow...
2025-05-07 23:17:23,785 - INFO - Initial GPU 0 memory usage: 20.96 GB
2025-05-07 23:17:37,683 - INFO - Run out-of-band function 'clean_worker_cache'
2025-05-07 23:17:46,932 - INFO - GPU 0 memory usage after fitting: 20.96 GB
2025-05-07 23:17:46,933 - INFO - Workflow fitting completed in 23.15 seconds
2025-05-07 23:17:46,933 - INFO - Transforming dataset...
2025-05-07 23:17:48,956 - INFO - Run out-of-band function 'clean_worker_cache'
Processing files:   0%|          | 0/1024 [00:00<?, ?it/s]Processing files:   0%|          | 0/1024 [00:00<?, ?it/s]
2025-05-07 23:17:49,391 - INFO - Closing Nanny at 'tcp://127.0.0.1:34733'. Reason: nanny-close
2025-05-07 23:17:49,391 - INFO - Nanny asking worker to close. Reason: nanny-close
2025-05-07 23:17:49,393 - INFO - Stopping worker at tcp://127.0.0.1:44071. Reason: nanny-close
2025-05-07 23:17:49,395 - INFO - Connection to tcp://127.0.0.1:42703 has been closed.
2025-05-07 23:17:49,395 - INFO - Received 'close-stream' from tcp://127.0.0.1:35434; closing.
2025-05-07 23:17:49,396 - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44071', name: 0, status: closing, memory: 0, processing: 0>
2025-05-07 23:17:49,396 - INFO - Removing comms to tcp://127.0.0.1:44071
2025-05-07 23:17:49,396 - INFO - Lost all workers
2025-05-07 23:17:50,545 - INFO - Scheduler closing...
2025-05-07 23:17:50,546 - INFO - Scheduler closing all comms
Traceback (most recent call last):
  File "/home/yuzhuyu/miniconda3/envs/nvtabular_23_08/lib/python3.10/site-packages/dask_cudf/io/parquet.py", line 276, in read_partition
    cls._read_paths(
  File "/home/yuzhuyu/miniconda3/envs/nvtabular_23_08/lib/python3.10/site-packages/dask_cudf/io/parquet.py", line 100, in _read_paths
    df = cudf.read_parquet(
  File "/home/yuzhuyu/miniconda3/envs/nvtabular_23_08/lib/python3.10/site-packages/nvtx/nvtx.py", line 122, in inner
    result = func(*args, **kwargs)
  File "/home/yuzhuyu/miniconda3/envs/nvtabular_23_08/lib/python3.10/site-packages/cudf/io/parquet.py", line 550, in read_parquet
    return _parquet_to_frame(
  File "/home/yuzhuyu/miniconda3/envs/nvtabular_23_08/lib/python3.10/site-packages/nvtx/nvtx.py", line 122, in inner
    result = func(*args, **kwargs)
  File "/home/yuzhuyu/miniconda3/envs/nvtabular_23_08/lib/python3.10/site-packages/cudf/io/parquet.py", line 579, in _parquet_to_frame
    return _read_parquet(
  File "/home/yuzhuyu/miniconda3/envs/nvtabular_23_08/lib/python3.10/site-packages/nvtx/nvtx.py", line 122, in inner
    result = func(*args, **kwargs)
  File "/home/yuzhuyu/miniconda3/envs/nvtabular_23_08/lib/python3.10/site-packages/cudf/io/parquet.py", line 688, in _read_parquet
    return libparquet.read_parquet(
  File "parquet.pyx", line 123, in cudf._lib.parquet.read_parquet
  File "parquet.pyx", line 182, in cudf._lib.parquet.read_parquet
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /home/yuzhuyu/miniconda3/envs/nvtabular_23_08/include/rmm/mr/device/cuda_memory_resource.hpp

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/yuzhuyu/u55c/rec_preprocessing/nvtabular_gpu_1TB_8k_no_vocab_dask.py", line 284, in <module>
    main()
  File "/home/yuzhuyu/u55c/rec_preprocessing/nvtabular_gpu_1TB_8k_no_vocab_dask.py", line 270, in main
    preprocess_data(train_paths, client, args.vocab_size, args.part_size)
  File "/home/yuzhuyu/u55c/rec_preprocessing/nvtabular_gpu_1TB_8k_no_vocab_dask.py", line 225, in preprocess_data
    for _ in transformed_data.to_iter():
  File "/home/yuzhuyu/miniconda3/envs/nvtabular_23_08/lib/python3.10/site-packages/merlin/io/dataframe_iter.py", line 44, in __iter__
    yield part.compute(scheduler="synchronous")
  File "/home/yuzhuyu/miniconda3/envs/nvtabular_23_08/lib/python3.10/site-packages/dask/base.py", line 314, in compute
    (result,) = compute(self, traverse=False, **kwargs)
  File "/home/yuzhuyu/miniconda3/envs/nvtabular_23_08/lib/python3.10/site-packages/dask/base.py", line 599, in compute
    results = schedule(dsk, keys, **kwargs)
  File "/home/yuzhuyu/miniconda3/envs/nvtabular_23_08/lib/python3.10/site-packages/dask/local.py", line 557, in get_sync
    return get_async(
  File "/home/yuzhuyu/miniconda3/envs/nvtabular_23_08/lib/python3.10/site-packages/dask/local.py", line 500, in get_async
    for key, res_info, failed in queue_get(queue).result():
  File "/home/yuzhuyu/miniconda3/envs/nvtabular_23_08/lib/python3.10/concurrent/futures/_base.py", line 451, in result
    return self.__get_result()
  File "/home/yuzhuyu/miniconda3/envs/nvtabular_23_08/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/home/yuzhuyu/miniconda3/envs/nvtabular_23_08/lib/python3.10/site-packages/dask/local.py", line 542, in submit
    fut.set_result(fn(*args, **kwargs))
  File "/home/yuzhuyu/miniconda3/envs/nvtabular_23_08/lib/python3.10/site-packages/dask/local.py", line 238, in batch_execute_tasks
    return [execute_task(*a) for a in it]
  File "/home/yuzhuyu/miniconda3/envs/nvtabular_23_08/lib/python3.10/site-packages/dask/local.py", line 238, in <listcomp>
    return [execute_task(*a) for a in it]
  File "/home/yuzhuyu/miniconda3/envs/nvtabular_23_08/lib/python3.10/site-packages/dask/local.py", line 229, in execute_task
    result = pack_exception(e, dumps)
  File "/home/yuzhuyu/miniconda3/envs/nvtabular_23_08/lib/python3.10/site-packages/dask/local.py", line 224, in execute_task
    result = _execute_task(task, data)
  File "/home/yuzhuyu/miniconda3/envs/nvtabular_23_08/lib/python3.10/site-packages/dask/core.py", line 119, in _execute_task
    return func(*(_execute_task(a, cache) for a in args))
  File "/home/yuzhuyu/miniconda3/envs/nvtabular_23_08/lib/python3.10/site-packages/dask/optimization.py", line 990, in __call__
    return core.get(self.dsk, self.outkey, dict(zip(self.inkeys, args)))
  File "/home/yuzhuyu/miniconda3/envs/nvtabular_23_08/lib/python3.10/site-packages/dask/core.py", line 149, in get
    result = _execute_task(task, cache)
  File "/home/yuzhuyu/miniconda3/envs/nvtabular_23_08/lib/python3.10/site-packages/dask/core.py", line 119, in _execute_task
    return func(*(_execute_task(a, cache) for a in args))
  File "/home/yuzhuyu/miniconda3/envs/nvtabular_23_08/lib/python3.10/site-packages/dask/core.py", line 119, in <genexpr>
    return func(*(_execute_task(a, cache) for a in args))
  File "/home/yuzhuyu/miniconda3/envs/nvtabular_23_08/lib/python3.10/site-packages/dask/core.py", line 113, in _execute_task
    return [_execute_task(a, cache) for a in arg]
  File "/home/yuzhuyu/miniconda3/envs/nvtabular_23_08/lib/python3.10/site-packages/dask/core.py", line 113, in <listcomp>
    return [_execute_task(a, cache) for a in arg]
  File "/home/yuzhuyu/miniconda3/envs/nvtabular_23_08/lib/python3.10/site-packages/dask/core.py", line 119, in _execute_task
    return func(*(_execute_task(a, cache) for a in args))
  File "/home/yuzhuyu/miniconda3/envs/nvtabular_23_08/lib/python3.10/site-packages/dask/dataframe/io/parquet/core.py", line 96, in __call__
    return read_parquet_part(
  File "/home/yuzhuyu/miniconda3/envs/nvtabular_23_08/lib/python3.10/site-packages/dask/dataframe/io/parquet/core.py", line 663, in read_parquet_part
    dfs = [
  File "/home/yuzhuyu/miniconda3/envs/nvtabular_23_08/lib/python3.10/site-packages/dask/dataframe/io/parquet/core.py", line 664, in <listcomp>
    func(
  File "/home/yuzhuyu/miniconda3/envs/nvtabular_23_08/lib/python3.10/site-packages/merlin/io/parquet.py", line 127, in read_partition
    return CudfEngine.read_partition(fs, pieces, *args, **kwargs)
  File "/home/yuzhuyu/miniconda3/envs/nvtabular_23_08/lib/python3.10/site-packages/dask_cudf/io/parquet.py", line 301, in read_partition
    raise MemoryError(
MemoryError: Parquet data was larger than the available GPU memory!

See the notes on split_row_groups in the read_parquet documentation.

Original Error: std::bad_alloc: out_of_memory: CUDA error at: /home/yuzhuyu/miniconda3/envs/nvtabular_23_08/include/rmm/mr/device/cuda_memory_resource.hpp
