python nvtabular_gpu_1TB_no_vocab.py --input_dir /mnt/scratch/yuzhuyu/parquet/criteo_1TB --part_mem_fraction 0.1
/home/yuzhuyu/miniconda3/envs/nvtabular/lib/python3.9/site-packages/merlin/dtypes/mappings/tf.py:52: UserWarning: Tensorflow dtype mappings did not load successfully due to an error: No module named 'tensorflow'
  warn(f"Tensorflow dtype mappings did not load successfully due to an error: {exc.msg}")
/home/yuzhuyu/miniconda3/envs/nvtabular/lib/python3.9/site-packages/merlin/dtypes/mappings/triton.py:53: UserWarning: Triton dtype mappings did not load successfully due to an error: No module named 'tritonclient'
  warn(f"Triton dtype mappings did not load successfully due to an error: {exc.msg}")
Found 1024 parquet files to process
Workflow creation completed in 0.00 seconds

Processing file: /mnt/scratch/yuzhuyu/parquet/criteo_1TB/criteo_1TB_part_0991.parquet
Dataset loading completed in 8.43 seconds
Fitting the workflow
Traceback (most recent call last):
  File "/home/yuzhuyu/miniconda3/envs/nvtabular/lib/python3.9/site-packages/dask_cudf/io/parquet.py", line 301, in read_partition
    cls._read_paths(
  File "/home/yuzhuyu/miniconda3/envs/nvtabular/lib/python3.9/site-packages/dask_cudf/io/parquet.py", line 112, in _read_paths
    df = cudf.read_parquet(
  File "/home/yuzhuyu/miniconda3/envs/nvtabular/lib/python3.9/site-packages/nvtx/nvtx.py", line 116, in inner
    result = func(*args, **kwargs)
  File "/home/yuzhuyu/miniconda3/envs/nvtabular/lib/python3.9/site-packages/cudf/io/parquet.py", line 577, in read_parquet
    df = _parquet_to_frame(
  File "/home/yuzhuyu/miniconda3/envs/nvtabular/lib/python3.9/site-packages/nvtx/nvtx.py", line 116, in inner
    result = func(*args, **kwargs)
  File "/home/yuzhuyu/miniconda3/envs/nvtabular/lib/python3.9/site-packages/cudf/io/parquet.py", line 721, in _parquet_to_frame
    return _read_parquet(
  File "/home/yuzhuyu/miniconda3/envs/nvtabular/lib/python3.9/site-packages/nvtx/nvtx.py", line 116, in inner
    result = func(*args, **kwargs)
  File "/home/yuzhuyu/miniconda3/envs/nvtabular/lib/python3.9/site-packages/cudf/io/parquet.py", line 829, in _read_parquet
    return libparquet.read_parquet(
  File "parquet.pyx", line 118, in cudf._lib.parquet.read_parquet
  File "parquet.pyx", line 174, in cudf._lib.parquet.read_parquet
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /home/yuzhuyu/miniconda3/envs/nvtabular/include/rmm/mr/device/cuda_memory_resource.hpp

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/yuzhuyu/u55c/rec_preprocessing/nvtabular_gpu_1TB_no_vocab.py", line 110, in <module>
    main()
  File "/home/yuzhuyu/u55c/rec_preprocessing/nvtabular_gpu_1TB_no_vocab.py", line 104, in main
    preprocess_criteo_parquet(input_files, frequency_threshold, args.part_mem_fraction)
  File "/home/yuzhuyu/u55c/rec_preprocessing/nvtabular_gpu_1TB_no_vocab.py", line 70, in preprocess_criteo_parquet
    workflow.fit(train_ds)
  File "/home/yuzhuyu/miniconda3/envs/nvtabular/lib/python3.9/site-packages/nvtabular/workflow/workflow.py", line 213, in fit
    self.executor.fit(dataset, self.graph)
  File "/home/yuzhuyu/miniconda3/envs/nvtabular/lib/python3.9/site-packages/merlin/dag/executors.py", line 479, in fit
    Dataset(
  File "/home/yuzhuyu/miniconda3/envs/nvtabular/lib/python3.9/site-packages/merlin/io/dataset.py", line 1262, in sample_dtypes
    _real_meta = self.engine.sample_data(n=n)
  File "/home/yuzhuyu/miniconda3/envs/nvtabular/lib/python3.9/site-packages/merlin/io/dataset_engine.py", line 71, in sample_data
    _head = _ddf.partitions[partition_index].head(n)
  File "/home/yuzhuyu/miniconda3/envs/nvtabular/lib/python3.9/site-packages/dask/dataframe/core.py", line 1590, in head
    return self._head(n=n, npartitions=npartitions, compute=compute, safe=safe)
  File "/home/yuzhuyu/miniconda3/envs/nvtabular/lib/python3.9/site-packages/dask/dataframe/core.py", line 1624, in _head
    result = result.compute()
  File "/home/yuzhuyu/miniconda3/envs/nvtabular/lib/python3.9/site-packages/dask/base.py", line 379, in compute
    (result,) = compute(self, traverse=False, **kwargs)
  File "/home/yuzhuyu/miniconda3/envs/nvtabular/lib/python3.9/site-packages/dask/base.py", line 665, in compute
    results = schedule(dsk, keys, **kwargs)
  File "/home/yuzhuyu/miniconda3/envs/nvtabular/lib/python3.9/site-packages/dask/dataframe/io/parquet/core.py", line 97, in __call__
    return read_parquet_part(
  File "/home/yuzhuyu/miniconda3/envs/nvtabular/lib/python3.9/site-packages/dask/dataframe/io/parquet/core.py", line 645, in read_parquet_part
    dfs = [
  File "/home/yuzhuyu/miniconda3/envs/nvtabular/lib/python3.9/site-packages/dask/dataframe/io/parquet/core.py", line 646, in <listcomp>
    func(
  File "/home/yuzhuyu/miniconda3/envs/nvtabular/lib/python3.9/site-packages/merlin/io/parquet.py", line 127, in read_partition
    return CudfEngine.read_partition(fs, pieces, *args, **kwargs)
  File "/home/yuzhuyu/miniconda3/envs/nvtabular/lib/python3.9/site-packages/dask_cudf/io/parquet.py", line 326, in read_partition
    raise MemoryError(
MemoryError: Parquet data was larger than the available GPU memory!

See the notes on split_row_groups in the read_parquet documentation.

Original Error: std::bad_alloc: out_of_memory: CUDA error at: /home/yuzhuyu/miniconda3/envs/nvtabular/include/rmm/mr/device/cuda_memory_resource.hpp
